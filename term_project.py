# -*- coding: utf-8 -*-
"""자연어처리 TERM PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A0_6xBRLptmzkd4z9839M9mnjbDSDE7q
"""

!pip install transformers
!pip install torch
!pip install konlpy
!pip install mecab-python
!pip install accelerate -U
!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

import re
import pandas as pd
import numpy as np
import urllib.request
from collections import Counter
from konlpy.tag import Mecab
from sklearn.model_selection import train_test_split
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments

# 데이터 로드
urllib.request.urlretrieve("https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt", filename="ratings_total.txt")
total_data = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])
print('전체 리뷰 개수 :', len(total_data))  # 전체 리뷰 개수 출력

# 라벨 생성
total_data['label'] = np.select([total_data.ratings > 3], [1], default=0)

# 데이터 전처리
total_data.drop_duplicates(subset=['reviews'], inplace=True)
print('총 샘플의 수 :', len(total_data))
print(total_data.isnull().values.any())

# 데이터 분리
train_data, test_data = train_test_split(total_data, test_size=0.25, random_state=42)
print('훈련용 리뷰의 개수 :', len(train_data))
print('테스트용 리뷰의 개수 :', len(test_data))

# 불용어 제거
stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']
mecab = Mecab()

def preprocess_text(text):
    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)
    tokens = mecab.morphs(text)
    tokens = [word for word in tokens if word not in stopwords]
    return ' '.join(tokens)

train_data['cleaned_reviews'] = train_data['reviews'].apply(preprocess_text)
test_data['cleaned_reviews'] = test_data['reviews'].apply(preprocess_text)

model_name = 'beomi/kcbert-base'  # 한글 BERT 모델
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

class SentimentDataset(torch.utils.data.Dataset):
    def __init__(self, reviews, labels, tokenizer, max_len):
        self.reviews = reviews
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.reviews)

    def __getitem__(self, idx):
        review = str(self.reviews[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            review,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            pad_to_max_length=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'review_text': review,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def create_data_loader(df, tokenizer, max_len, batch_size):
    ds = SentimentDataset(
        reviews=df.cleaned_reviews.to_numpy(),
        labels=df.label.to_numpy(),
        tokenizer=tokenizer,
        max_len=max_len
    )

    return torch.utils.data.DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=2
    )

BATCH_SIZE = 64
MAX_LEN = 160

train_data_loader = create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE)

training_args = TrainingArguments(
  output_dir='./results',
  num_train_epochs=1,
  per_device_train_batch_size=BATCH_SIZE,
  per_device_eval_batch_size=BATCH_SIZE,
  warmup_steps=500,
  weight_decay=0.01,
  logging_dir='./logs',
  logging_steps=10,
  evaluation_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data_loader.dataset,
    eval_dataset=test_data_loader.dataset,
    tokenizer=tokenizer
)

trainer.train()

trainer.save_model('./saved_model')

trainer.evaluate()

def sentiment_predict(sentence):
    sentence = preprocess_text(sentence)
    encoding = tokenizer.encode_plus(
        sentence,
        add_special_tokens=True,
        max_length=MAX_LEN,
        return_token_type_ids=False,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors='pt',
    )

    input_ids = encoding['input_ids'].to(model.device)
    attention_mask = encoding['attention_mask'].to(model.device)

    outputs = model(input_ids, attention_mask=attention_mask)
    _, prediction = torch.max(outputs.logits, dim=1)

    print(f'Review: {sentence}')
    print(f'Sentiment: {"Positive" if prediction else "Negative"}')

# 예시 예측
sentiment_predict('이 상품 진짜 좋아요... 저는 강추합니다. 대박')
sentiment_predict('진짜 배송도 늦고 개짜증나네요. 뭐 이런 걸 상품이라고 만듬?')

!zip -r /content/saved_model.zip /content/saved_model

!pip install streamlit transformers pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
# 
# # Streamlit 웹 애플리케이션 제목
# st.title("Sentiment Analysis with Huggingface")
# 
# # 사용자 입력을 위한 텍스트 박스
# user_input = st.text_area("Enter text to analyze:")
# 
# # 모델과 토크나이저 로드
# model_name = "distilbert-base-uncased-finetuned-sst-2-english"
# tokenizer = AutoTokenizer.from_pretrained('/content/saved_model')
# model = AutoModelForSequenceClassification.from_pretrained('/content/saved_model')
# 
# # 파이프라인 생성
# nlp = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
# 
# # 분석 버튼
# if st.button("Analyze"):
#     if user_input:
#         # 감정 분석 수행
#         result = nlp(user_input)
#         label = result[0]['label']
#         score = result[0]['score']
# 
#         # 결과 출력
#         if label == 'POSITIVE':
#             st.write(f"Review: {user_input}")
#             st.write(f"Sentiment: Positive")
#         else:
#             st.write(f"Review: {user_input}")
#             st.write(f"Sentiment: Negative")
#     else:
#         st.write("Please enter text to analyze.")
#

import urllib
print("Password/Enpoint IP for localtunnel is:",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))

!streamlit run app.py &>/content/logs.txt &
!npx localtunnel --port 8501